{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 마이닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "- [텍스트 마이닝의 이해](#paragraph1)\n",
    "- [텍스트 마이닝 방법론](#paragraph2)\n",
    "- [텍스트 마이닝의 문제점](#paragraph3)\n",
    "- [문제 해결을 위한 방안](#paragraph4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝의 이해 <a name=\"paragraph1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 마이닝이란\n",
    "\n",
    "텍스트 마이닝(text mining)은 언어학, 통계학, 기계 학습 등을 기반으로 한 자연언어 처리 기술을 활용하여 반정형 및 비정형 텍스트 데이터를 정형화하고, 특징을 추출하기 위한 기술과 추출된 특징으로부터 의미 있는 정보를 발견할 수 있도록 하는 기술이다.\n",
    "<br>(출처: [해시넷](http://wiki.hash.kr/index.php/%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D))\n",
    "\n",
    "<br>\n",
    "\n",
    "텍스트 마이닝의 주요 기술은 다음과 같다\n",
    "\n",
    "**자연어처리**: 자연어처리 기술(NLP, Natural Language Processing)이란, 워드 임베딩과 같은 기술을 사용해 컴퓨터가 인간의 언어를 알아들을 수 있게 만드는 학문 분야이다.\n",
    "\n",
    "**통계학, 선형대수**: 여러 통계기법들과 벡터, 행렬 연산을 통해 텍스트 데이터를 분석가능하게 변환시킨다.\n",
    "\n",
    "**머신러닝**: 나이브 베이즈, 로지스틱 회귀분석, 의사결정트리, SVM (Support Vector Machine) 등 여러 알고리즘을 사용한다.\n",
    "\n",
    "**딥러닝**: RNN (Recurrent Neural Network), LSTM (Long Short-Term Memory), Attention, Transformer, Bert, GPT 와 같은 딥러닝 기법을 사용한다\n",
    "\n",
    "<br>\n",
    "\n",
    "텍스트 마이닝은 여러 분야에서 활발하게 연구가 이뤄지고 있고 사용되고 있다.\n",
    "\n",
    "**텍스트 분류**: 대표적으로 감성분석에 사용된다.\n",
    "\n",
    "**텍스트 생성**: 뉴스 생성, QnA, 번역과 같은 곳에서 사용된다.\n",
    "\n",
    "**키워드 추출**: 텍스트에 태그나 카테고리를 구분할 때 사용된다.\n",
    "\n",
    "**토픽 모델링**: 텍스트의 특정 주제나 이슈, 주제 그룹들을 추출할 때 사용된다.\n",
    "\n",
    "<br>\n",
    "\n",
    "파이썬은 풍부한 라이브러리로 인해 텍스트 마이닝에 가장 많이 사용되는 언어이다. 가장 많이 알려진 NLP 라이브러리인 **NLTK**, 머신러닝 라이브러리와 기본적인 NLP와 다양한 텍스트 마이닝 관련 도구를 지원하는 **scikit learn**, Word2Vec으로 유명한 **gensim**, RNN, seq2seq 등 딥러닝 위주의 라이브러리를 제공하는 **keras** 등이 있다. 참고로 **keras**는 **tensorflow** 라이브러리의 high-level API다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝 방법론 <a name=\"paragraph2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 마이닝은 텍스트를 벡터로 변환하는것으로 시작한다. 벡터로 변환하는데 사용되는 기술들이 있는데 같이 살펴보기로 하자.\n",
    "\n",
    "### Tokenize\n",
    "\n",
    "하나의 텍스트를 단어들로 분리 시켜주는 단계이다. 이때 의미 없는 문자들을 걸러낼 수도 있다. \n",
    "\n",
    "영어와 한글의 tokenize는 차이가 있는데 영어는 문장 구조상 단어와 단어 사이에는 공백이 존재한다. 하지만 한글은 조사와 어미 등을 구별해야하는 작업이 추가로 필요하다. 그래서 한글 텍스트 마이닝이 영어보다 더 어렵다고 알려져있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank', 'God', 'it', \"'s\", 'friday', '!']\n"
     ]
    }
   ],
   "source": [
    "# 영어\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "\n",
    "text = \"Thank God it's friday!\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['애플', '이', '2년', '만에', '새로운', '아이패드', '미니', '를', '공개', '했다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 한글\n",
    "from konlpy.tag import Okt  \n",
    "okt = Okt()\n",
    "\n",
    "text = \"애플이 2년 만에 새로운 아이패드 미니를 공개했다.\"\n",
    "print(okt.morphs(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "\n",
    "Normalization은 정규화 라는 뜻을 가지고 있다. Text normalization이란 말그대로 단어들을 정규화 시켜준다는 것인데, 같은 의미의 단어가 다른 형태를 가지고 있는 경우 이를 같은 형태로 변환시켜주는 것이다. 그럼 어떻게 텍스트를 정규화 시킬 수 있을까? 바로 Stemming 과 lemmatization 기법이 사용된다. \n",
    "\n",
    "**Stemming**\n",
    "\n",
    "어간(stem) 추출이라고도 하고 단수-복수, 현재형-미래형 등 단어의 다양한 변형을 하나로 통일하는 기법이다. 이때 단어의 의미가 아닌 규칙(알고리즘)에 의해 변환시킨다. Porter stemmer와 Lancaster stemmer가 유명하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Global investors were in the dark about whether the property giant would make $83.5 million in payments, \n",
      "a major test of the highly indebted developer’s ability to avoid a default.\n",
      "\n",
      "Stemmed:\n",
      "global investor were in the dark about whether the properti giant would make $ 83.5 million in payment , a major test of the highli indebt develop ’ s abil to avoid a default .\n"
     ]
    }
   ],
   "source": [
    "def stem_sentence(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence = []\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "    return \" \".join(stem_sentence)\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "sentence = '''Global investors were in the dark about whether the property giant would make $83.5 million in payments, \n",
    "a major test of the highly indebted developer’s ability to avoid a default.'''\n",
    "\n",
    "print('Original:')\n",
    "print(sentence)\n",
    "print('\\nStemmed:')\n",
    "print(stem_sentence(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "표제어(lemma) 추출이라고도 하고 사전을 이용하여 단어의 원형을 추출하는 기법이다. WordNet lemmatizer가 유명하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/cho2jiwoo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\n",
      "\n",
      "Lemmatized:\n",
      "He wa running and eating at same time . He ha bad habit of swimming after playing long hour in the Sun .\n"
     ]
    }
   ],
   "source": [
    "def lemmatize(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    token_words\n",
    "    lemmatize_sentence = []\n",
    "    for word in token_words:\n",
    "        lemmatize_sentence.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    return \" \".join(lemmatize_sentence)\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "print('Original:')\n",
    "print(sentence)\n",
    "print('\\nLemmatized:')\n",
    "print(lemmatize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-tagging\n",
    "\n",
    "POS (Part of Speech) tagging, 즉 품사 태깅은 형태소의 뜻과 문맥을 고려하여 그것에 마크업을 하는 일이다.예를 들어:\n",
    "<br>\n",
    "`가방에 들어가신다 -> 가방/NNG + 에/JKM + 들어가/VV + 시/EPH + ㄴ다/EFN`\n",
    "<br>\n",
    "(출처: [KoNLPy 공식 문서](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/))\n",
    "\n",
    "동일한 단어라도 문맥에 따라 의미가 달라지므로 품사를 알기 위해서는 문맥을 파악해야 한다. 한글에 특수성 때문에 한글에 특화된 형태소 분석기를 사용해야 좋은 결과가 나올 수 있다. `konlpy` 패키지에는 여러 형태소 분석기가 들어있는데 예제에서는 카이스트 semantic web research center에서 만든 Hannanum을 사용해보자. 다른 형태소 분석기 들은 [이곳](https://mr-doosun.tistory.com/22)을 참고."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('하늘', 'N'), ('을', 'J'), ('나', 'N'), ('는', 'J'), ('자동차', 'N')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum()\n",
    "sentence = '하늘을 나는 자동차'\n",
    "print(hannanum.pos(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"하늘을 나는 자동차\" 라는 문장에서 \"나\"를 \"me\"로 인식하고 명사로 태그한것을 볼 수 있다. 이 문장에서는 `나(-ㄹ다)/V + 는/E` 이 되는 것이 바람직하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "Chunk는 언어학적으로 말모듬을 뜻하며, 명사구, 형용사구, 분사구 등과 같이 주어와 동사가 없는 두 단어 이상의 집합인 구(phrase)를 의미한다. Chunking은 문장에서 구(phrase)를 찾는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"The little yellow dog barked at the cat\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp  = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tag)\n",
    "# result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.nltk.org/book/tree_images/ch07-tree-1.png)\n",
    "\n",
    "<div align='center'>reference: https://www.nltk.org/book/ch07.html</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER\n",
    "\n",
    "NER(Named Entity Recognition)은 텍스트로 부터 의미 있는 정보를 추출하기 위한 방법으로 사용된다. NER에 NE는 Named Entity, 개체명이라 부르고 기관, 단체, 사람, 날짜 등과 같이 특정 정보에 해당하는 명사구를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    European\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " authorities fined \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " a record \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $5.1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wednesday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " for </br>abusing its power in the mobile phone market and ordered the company to alter its practices</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "# Run this command in terminal\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "sentence = '''European authorities fined Google a record $5.1 billion on Wednesday for \n",
    "abusing its power in the mobile phone market and ordered the company to alter its practices'''\n",
    "ners = NER(sentence)\n",
    "displacy.render(ners, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://www.analyticsvidhya.com/blog/2021/06/nlp-application-named-entity-recognition-ner-in-python-with-spacy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW\n",
    "\n",
    "BOW(Bag of Words)란 단어들의 순서를 고려하지 않고, 단어들의 출현 빈도(frequency)만 가지고 vector로 표현하는 방법이다. scikit learn 라이브러리에 CountVectorizer을 사용하면 간단히 bag of words를 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://wikidocs.net/22650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "\n",
    "TF-IDF(Term Frequency-Inverse Document Frequency)는 단어의 빈도와 역 문서 빈도를 사용하여 DTM(Document Term Matrix) 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법이다.\n",
    "\n",
    "TF-IDF는 TF와 IDF를 곱한 값이다.\n",
    "\n",
    "$d = 문서$\n",
    "<br>\n",
    "$t = 단어$\n",
    "<br>\n",
    "$n = 총 \\ 문서 \\ 수$\n",
    "\n",
    "$tf(d, t)$ : 특정 문서 d에서의 특정 단어 t의 등장 횟수.\n",
    "\n",
    "$df(t)$ : 특정 단어 t가 등장한 문서의 수.\n",
    "\n",
    "$idf(d, t)$ : df(t)에 반비례하는 수.\n",
    "\n",
    "<br>\n",
    "\n",
    "TFIDF도 scikit learn 라이브러리를 이용해 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.355432</td>\n",
       "      <td>0.467351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.605349</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3        4         5        6  \\\n",
       "0  0.00000  0.467351  0.000000  0.467351  0.00000  0.467351  0.00000   \n",
       "1  0.00000  0.000000  0.795961  0.000000  0.00000  0.000000  0.00000   \n",
       "2  0.57735  0.000000  0.000000  0.000000  0.57735  0.000000  0.57735   \n",
       "\n",
       "          7         8  \n",
       "0  0.355432  0.467351  \n",
       "1  0.605349  0.000000  \n",
       "2  0.000000  0.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',\n",
    "]\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "df = pd.DataFrame(tfidfv.transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://wikidocs.net/31698"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**코사인 유사도**\n",
    "\n",
    "TFIDF를 이용하여 유사도를 계산할 수 있다. 코사인 유사도랑 두 벡터 간의 코사인 각도를 구하여 두 벡터의 유사도를 나타낼 수 있다. 코사인 유사도 값이 1에 가까울 수록 유사도가 높다고 판단할 수 있다.\n",
    "\n",
    "코사인 유사도는 다음과 같이 구할 수 있다.\n",
    "\n",
    "$$\n",
    "similarity=cos(Θ)=\\frac{A⋅B}{||A||\\ ||B||}=\\frac{\\sum_{i=1}^{n}{A_{i}×B_{i}}}{\\sqrt{\\sum_{i=1}^{n}(A_{i})^2}×\\sqrt{\\sum_{i=1}^{n}(B_{i})^2}}\n",
    "$$\n",
    "\n",
    "![](https://wikidocs.net/images/page/24603/%EC%BD%94%EC%82%AC%EC%9D%B8%EC%9C%A0%EC%82%AC%EB%8F%84.PNG)\n",
    "\n",
    "<div align='center'>참고: https://wikidocs.net/24603</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification with BOW/TFIDF\n",
    "\n",
    "BOW 와 TFIDF 기법으로 텍스를 벡터화 했다면 여러 classification 알고리즘들로 텍스트 분류를 할 수 있게 된다. 여러가지 알고리즘이 있지만 여기선 **Naive Bayes** 와 **Logistic regression**에 대해 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**\n",
    "\n",
    "나이브 베이지안(Naive Bayesian) 알고리즘은 베이즈 정리를 이용한 확률적 기계학습 알고리즘이다. 사전 확률에 기반을 두고 사후 확률을 추론하는 확률적 예측을 하는데, 이 때 모든 사건이 독립사건이라는 순진한(naive) 가정을 하고 있기 때문에 나이브 베이지안이라는 이름을 가지게 되었다.\n",
    "\n",
    "두 개의 사건 A와 B에 대해서 조건부 확률에 대한 베이즈 정리는 다음과 같다.\n",
    "$$\n",
    "\\displaystyle \n",
    "P(A|B)={P(B|A)P(A)\\over P(B)}\n",
    "$$\n",
    "\n",
    "여기에서 P(A)P(A)를 사전 확률(prior probability)라 하고, P(A|B)P(A∣B)를 사후 확률(posterior probability)이라 한다. 베이즈 정리는 사전 확률을 통해서 사후 확률을 예측하거나 추론하는데 사용할 수 있다는 점에서 의미가 있다.\n",
    "\n",
    "나이브 베이지안 알고리즘은 스팸메일 필터, 텍스트 분석기, 추천 시스템, 의학적 질병 진단 등의 광범위한 분야에서 예측과 추론을 위한 분류기로 많이 활용되고 있다.\n",
    "\n",
    "출처: [나무위키](https://namu.wiki/w/%EB%82%98%EC%9D%B4%EB%B8%8C%20%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88%20%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression**\n",
    "\n",
    "로지스틱 회귀분석은 예측변수 세트의 값을 기준으로 결정되는 특성이나 결과가 있는지 여부를 예측하려는 상황에서 유용하다. 이 회귀분석은 선형 회귀 모형과 유사하나 종속변수가 이분형인 모형에 적합하다.\n",
    "<br>(출처: [IBM](https://www.ibm.com/docs/ko/spss-statistics/25.0.0?topic=regression-logistic))\n",
    "\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/83016hmfile_hash_38a8acae.png\" alt=\"Logistic Regression\" width=\"50%\" height=\"50%\" />\n",
    "\n",
    "<div align='center'>출처: https://www.analyticsvidhya.com/blog/2021/05/logistic-regression-supervised-learning-algorithm-for-classification/</div>\n",
    "\n",
    "로지스틱 회귀분석으로 텍스트 분류를 할 때는 추정해야 할 계수가 벡터의 크기만큼 존재하므로 과적합(overfitting)이 발생하기 쉽고 이를 해결하기 위해선 많은 데이터 셋이 필요하거나 정규화를 통해 이를 해결할 수 있다. 일반적으로 **Ridge regression**과 **Lasso regression**을 사용하여 정규화를 하게 된다.\n",
    "\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F7vchA%2FbtqDjeASZK5%2F7V4fGRlesJtu67v2mTgIOK%2Fimg.png)\n",
    "\n",
    "<div align='center'>출처: https://rk1993.tistory.com/entry/Ridge-regression%EC%99%80-Lasso-regression-쉽게-이해하기</div>\n",
    "\n",
    "|정규화|방식|\n",
    "|:---:|:---:|\n",
    "|Ridge regression|영향을 거의 미치지 않는 특성에 대하여 0에 가가운 가중치를 준다|\n",
    "|Lasso regression|특성값의 계수가 매우 낮다면 0으로 수렴하게 하여 특성을 지워버린다|\n",
    "\n",
    "<div align='center'>reference: https://hwiyong.tistory.com/93</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 분류 응용\n",
    "\n",
    "**Sentiment Analysis**\n",
    "\n",
    "감성분석이라고 알려져 있는 sentiment analysis는 텍스트에서 감정을 추출하기 위한 분석이다. \n",
    "\n",
    "나이브 베이즈 알고리즘을 사용해 간단한 영화 리뷰의 감성을 분석을 해보자. 간단한 분석을 위해 따로 모델을 개선시키지 않았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(f'accuracy: {nltk.classify.accuracy(classifier, test_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment: neg\n"
     ]
    }
   ],
   "source": [
    "# from actual review of Parasite (2019)\n",
    "# https://www.imdb.com/review/rw5426376/?ref_=ext_shr_lnk\n",
    "review = \"\"\"Parasite is a unique movie. I don't know if it's a true masterpiece, \n",
    "or one of a kind (indeed), or just a piece of masterful craft. \n",
    "Bong does everything he wants while telling his stories, and he's the only one \n",
    "I know who can do that aside from Tarantino. The plus here is a very sound moral. \n",
    "I will think a lot about Parasite, and maybe watch it again. This is the first time in \n",
    "100+ reviews that I don't feel I can correctly rate a movie, but surely it is in the 8 to 10 stars range.\"\"\"\n",
    "\n",
    "print(f'sentiment: {classifier.classify(document_features(review))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://www.datacamp.com/community/tutorials/simplifying-sentiment-analysis-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝의 문제점 <a name=\"paragraph3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 마이닝은 텍스트를 연산 가능한 수치로 변환해야 하기 때문에 일반적인 수치 데이터들을 분석할 때 와는 다른 문제점들이 있다. 여기선 3가지 문제점들을 살펴보고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curse of dimensionality**\n",
    "\n",
    "차원의 저주라고 불리는 curse of dimensionality 문제는 텍스트를 벡터화 시켰을 때 생기는 차원이 매우 커서 학습할 데이터의 수보다 커지며 발생한다. 그리고 sparse data일 수록 차원은 더욱 커지며 이렇게 학습된 모델은 성능이 좋지 않아진다. 차원의 저주를 해결하기 위해선 더 많은 데이터를 학습시키거나 차원을 축소(dimension reduction)하는 과정을 거쳐야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단어 빈도의 불균형**\n",
    "\n",
    "문서에서 단어별 빈도수는 아래 그림과 long right tail 형태를 띄게 된다.\n",
    "\n",
    "<img src=\"https://afit-r.github.io/public/images/analytics/descriptives/tf_idf1.png\" alt=\"word frequency\" width=\"50%\" height=\"60%\"/>\n",
    "\n",
    "<div align='center'>출처: https://afit-r.github.io/tf-idf_analysis</div>\n",
    "\n",
    "Zipf's law에 따르면 적은 빈도의 단어일수록 문서내에서 더 중요한 단어일것이라고 하였다. 그렇기 때문에 빈도 높은 단어를 삭제해준다거나 데이터에 함수를 씌워(root, log 등) 데이터를 변환시켜주어 이러한 문제점을 해결할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단어가 쓰인 순서 정보의 손실**\n",
    "\n",
    "정통적인 BOW 나 TFIDF는 문서를 단어들로 쪼개어 벡터로 만들었다. 그렇게 되면 문서가 가지고 있는 문맥을 잃게 되어 문장의 의미를 파악하기 어려워진다. 앞서 품사 태깅의 예재에서 봤던것 처럼 \"하늘을 나는 자동차\"를 토크나이즈하게 되면 문장의 의미를 잃게 되어 제대로 된 품사 태깅을 하지 못하게 된다. \"나는\" 이란 말은 이 문장에서는 \"하늘을\" 이란 단어와 함께 봐야 정확한 의미를 파악할 수 있다. 이러한 문제들을 해결하기 위해 word embedding 기법과 deep learning 기법들이 연구되었고 대표적으로 RNN, Attention, Transformer의 등장으로 텍스트 마이닝에 많은 발전을 이뤄냈다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 해결을 위한 방안 <a name=\"paragraph4\"></a>\n",
    "\n",
    "기존의 텍스트 마이닝 기법의 한계점을 확인해 보았으니 앞서 다뤘던 문제들을 해결하기 위해 나온 여러 방법들을 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5bda608a311093dc80a7df19f3902d093b81111c301cdabb1e2f80ec2133e97f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
