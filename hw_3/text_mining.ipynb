{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 마이닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "- [텍스트 마이닝의 이해](#paragraph1)\n",
    "- [텍스트 마이닝 방법론](#paragraph2)\n",
    "- [텍스트 마이닝의 문제점](#paragraph3)\n",
    "- [문제 해결을 위한 방안](#paragraph4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝의 이해 <a name=\"paragraph1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 마이닝이란\n",
    "\n",
    "텍스트 마이닝(text mining)은 언어학, 통계학, 기계 학습 등을 기반으로 한 자연언어 처리 기술을 활용하여 반정형 및 비정형 텍스트 데이터를 정형화하고, 특징을 추출하기 위한 기술과 추출된 특징으로부터 의미 있는 정보를 발견할 수 있도록 하는 기술이다.\n",
    "<br>(출처: [해시넷](http://wiki.hash.kr/index.php/%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D))\n",
    "\n",
    "<br>\n",
    "\n",
    "텍스트 마이닝의 주요 기술은 다음과 같다\n",
    "\n",
    "**자연어처리**: 자연어처리 기술(NLP, Natural Language Processing)이란, 워드 임베딩과 같은 기술을 사용해 컴퓨터가 인간의 언어를 알아들을 수 있게 만드는 학문 분야이다.\n",
    "\n",
    "**통계학, 선형대수**: 여러 통계기법들과 벡터, 행렬 연산을 통해 텍스트 데이터를 분석가능하게 변환시킨다.\n",
    "\n",
    "**머신러닝**: 나이브 베이즈, 로지스틱 회귀분석, 의사결정트리, SVM (Support Vector Machine) 등 여러 알고리즘을 사용한다.\n",
    "\n",
    "**딥러닝**: RNN (Recurrent Neural Network), LSTM (Long Short-Term Memory), Attention, Transformer, Bert, GPT 와 같은 딥러닝 기법을 사용한다\n",
    "\n",
    "<br>\n",
    "\n",
    "텍스트 마이닝은 여러 분야에서 활발하게 연구가 이뤄지고 있고 사용되고 있다.\n",
    "\n",
    "**텍스트 분류**: 대표적으로 감성분석에 사용된다.\n",
    "\n",
    "**텍스트 생성**: 뉴스 생성, QnA, 번역과 같은 곳에서 사용된다.\n",
    "\n",
    "**키워드 추출**: 텍스트에 태그나 카테고리를 구분할 때 사용된다.\n",
    "\n",
    "**토픽 모델링**: 텍스트의 특정 주제나 이슈, 주제 그룹들을 추출할 때 사용된다.\n",
    "\n",
    "<br>\n",
    "\n",
    "파이썬은 풍부한 라이브러리로 인해 텍스트 마이닝에 가장 많이 사용되는 언어이다. 가장 많이 알려진 NLP 라이브러리인 **NLTK**, 머신러닝 라이브러리와 기본적인 NLP와 다양한 텍스트 마이닝 관련 도구를 지원하는 **scikit learn**, Word2Vec으로 유명한 **gensim**, RNN, seq2seq 등 딥러닝 위주의 라이브러리를 제공하는 **keras** 등이 있다. 참고로 **keras**는 **tensorflow** 라이브러리의 high-level API다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝 방법론 <a name=\"paragraph2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 마이닝은 텍스트를 벡터로 변환하는것으로 시작한다. 벡터로 변환하는데 사용되는 기술들이 있는데 같이 살펴보기로 하자.\n",
    "\n",
    "### Tokenize\n",
    "\n",
    "하나의 텍스트를 단어들로 분리 시켜주는 단계이다. 이때 의미 없는 문자들을 걸러낼 수도 있다. \n",
    "\n",
    "영어와 한글의 tokenize는 차이가 있는데 영어는 문장 구조상 단어와 단어 사이에는 공백이 존재한다. 하지만 한글은 조사와 어미 등을 구별해야하는 작업이 추가로 필요하다. 그래서 한글 텍스트 마이닝이 영어보다 더 어렵다고 알려져있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank', 'God', 'it', \"'s\", 'friday', '!']\n"
     ]
    }
   ],
   "source": [
    "# 영어\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "\n",
    "text = \"Thank God it's friday!\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['애플', '이', '2년', '만에', '새로운', '아이패드', '미니', '를', '공개', '했다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 한글\n",
    "from konlpy.tag import Okt  \n",
    "okt = Okt()\n",
    "\n",
    "text = \"애플이 2년 만에 새로운 아이패드 미니를 공개했다.\"\n",
    "print(okt.morphs(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "\n",
    "Normalization은 정규화 라는 뜻을 가지고 있다. Text normalization이란 말그대로 단어들을 정규화 시켜준다는 것인데, 같은 의미의 단어가 다른 형태를 가지고 있는 경우 이를 같은 형태로 변환시켜주는 것이다. 그럼 어떻게 텍스트를 정규화 시킬 수 있을까? 바로 Stemming 과 lemmatization 기법이 사용된다. \n",
    "\n",
    "**Stemming**\n",
    "\n",
    "어간(stem) 추출이라고도 하고 단수-복수, 현재형-미래형 등 단어의 다양한 변형을 하나로 통일하는 기법이다. 이때 단어의 의미가 아닌 규칙(알고리즘)에 의해 변환시킨다. Porter stemmer와 Lancaster stemmer가 유명하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Global investors were in the dark about whether the property giant would make $83.5 million in payments, \n",
      "a major test of the highly indebted developer’s ability to avoid a default.\n",
      "\n",
      "Stemmed:\n",
      "global investor were in the dark about whether the properti giant would make $ 83.5 million in payment , a major test of the highli indebt develop ’ s abil to avoid a default .\n"
     ]
    }
   ],
   "source": [
    "def stem_sentence(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence = []\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "    return \" \".join(stem_sentence)\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "sentence = '''Global investors were in the dark about whether the property giant would make $83.5 million in payments, \n",
    "a major test of the highly indebted developer’s ability to avoid a default.'''\n",
    "\n",
    "print('Original:')\n",
    "print(sentence)\n",
    "print('\\nStemmed:')\n",
    "print(stem_sentence(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "표제어(lemma) 추출이라고도 하고 사전을 이용하여 단어의 원형을 추출하는 기법이다. WordNet lemmatizer가 유명하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/cho2jiwoo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\n",
      "\n",
      "Lemmatized:\n",
      "He wa running and eating at same time . He ha bad habit of swimming after playing long hour in the Sun .\n"
     ]
    }
   ],
   "source": [
    "def lemmatize(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    token_words\n",
    "    lemmatize_sentence = []\n",
    "    for word in token_words:\n",
    "        lemmatize_sentence.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    return \" \".join(lemmatize_sentence)\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "print('Original:')\n",
    "print(sentence)\n",
    "print('\\nLemmatized:')\n",
    "print(lemmatize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-tagging\n",
    "\n",
    "POS (Part of Speech) tagging, 즉 품사 태깅은 형태소의 뜻과 문맥을 고려하여 그것에 마크업을 하는 일이다.예를 들어:\n",
    "<br>\n",
    "`가방에 들어가신다 -> 가방/NNG + 에/JKM + 들어가/VV + 시/EPH + ㄴ다/EFN`\n",
    "<br>\n",
    "(출처: [KoNLPy 공식 문서](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/))\n",
    "\n",
    "동일한 단어라도 문맥에 따라 의미가 달라지므로 품사를 알기 위해서는 문맥을 파악해야 한다. 한글에 특수성 때문에 한글에 특화된 형태소 분석기를 사용해야 좋은 결과가 나올 수 있다. `konlpy` 패키지에는 여러 형태소 분석기가 들어있는데 예제에서는 카이스트 semantic web research center에서 만든 Hannanum을 사용해보자. 다른 형태소 분석기 들은 [이곳](https://mr-doosun.tistory.com/22)을 참고."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('하늘', 'N'), ('을', 'J'), ('나', 'N'), ('는', 'J'), ('자동차', 'N')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum()\n",
    "sentence = '하늘을 나는 자동차'\n",
    "print(hannanum.pos(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"하늘을 나는 자동차\" 라는 문장에서 \"나\"를 \"me\"로 인식하고 명사로 태그한것을 볼 수 있다. 이 문장에서는 `나(-ㄹ다)/V + 는/E` 이 되는 것이 바람직하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "Chunk는 언어학적으로 말모듬을 뜻하며, 명사구, 형용사구, 분사구 등과 같이 주어와 동사가 없는 두 단어 이상의 집합인 구(phrase)를 의미한다. Chunking은 문장에서 구(phrase)를 찾는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"The little yellow dog barked at the cat\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp  = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tag)\n",
    "# result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.nltk.org/book/tree_images/ch07-tree-1.png)\n",
    "\n",
    "reference: https://www.nltk.org/book/ch07.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER\n",
    "\n",
    "NER(Named Entity Recognition)은 텍스트로 부터 의미 있는 정보를 추출하기 위한 방법으로 사용된다. NER에 NE는 Named Entity, 개체명이라 부르고 기관, 단체, 사람, 날짜 등과 같이 특정 정보에 해당하는 명사구를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    European\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " authorities fined \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " a record \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $5.1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wednesday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " for </br>abusing its power in the mobile phone market and ordered the company to alter its practices</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "# Run this command in terminal\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "sentence = '''European authorities fined Google a record $5.1 billion on Wednesday for \n",
    "abusing its power in the mobile phone market and ordered the company to alter its practices'''\n",
    "ners = NER(sentence)\n",
    "displacy.render(ners, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://www.analyticsvidhya.com/blog/2021/06/nlp-application-named-entity-recognition-ner-in-python-with-spacy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW\n",
    "\n",
    "BOW(Bag of Words)란 단어들의 순서를 고려하지 않고, 단어들의 출현 빈도(frequency)만 가지고 vector로 표현하는 방법이다. scikit learn 라이브러리에 CountVectorizer을 사용하면 간단히 bag of words를 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://wikidocs.net/22650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "\n",
    "TF-IDF(Term Frequency-Inverse Document Frequency)는 단어의 빈도와 역 문서 빈도를 사용하여 DTM(Document Term Matrix) 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법이다.\n",
    "\n",
    "TF-IDF는 TF와 IDF를 곱한 값이다.\n",
    "\n",
    "$d = 문서$\n",
    "<br>\n",
    "$t = 단어$\n",
    "<br>\n",
    "$n = 총 \\ 문서 \\ 수$\n",
    "\n",
    "$tf(d, t)$ : 특정 문서 d에서의 특정 단어 t의 등장 횟수.\n",
    "\n",
    "$df(t)$ : 특정 단어 t가 등장한 문서의 수.\n",
    "\n",
    "$idf(d, t)$ : df(t)에 반비례하는 수.\n",
    "\n",
    "<br>\n",
    "\n",
    "TFIDF도 scikit learn 라이브러리를 이용해 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.355432</td>\n",
       "      <td>0.467351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.605349</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3        4         5        6  \\\n",
       "0  0.00000  0.467351  0.000000  0.467351  0.00000  0.467351  0.00000   \n",
       "1  0.00000  0.000000  0.795961  0.000000  0.00000  0.000000  0.00000   \n",
       "2  0.57735  0.000000  0.000000  0.000000  0.57735  0.000000  0.57735   \n",
       "\n",
       "          7         8  \n",
       "0  0.355432  0.467351  \n",
       "1  0.605349  0.000000  \n",
       "2  0.000000  0.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',\n",
    "]\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "df = pd.DataFrame(tfidfv.transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://wikidocs.net/31698"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**코사인 유사도**\n",
    "\n",
    "TFIDF를 이용하여 유사도를 계산할 수 있다.\n",
    "\n",
    "`여기서 부터 진행...`\n",
    "참고: https://wikidocs.net/24603\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝의 문제점 <a name=\"paragraph3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 해결을 위한 방안 <a name=\"paragraph4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5bda608a311093dc80a7df19f3902d093b81111c301cdabb1e2f80ec2133e97f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
